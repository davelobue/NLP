{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.1 \n",
    "# Document Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_directory = '///////philosophy/corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Heuristic Classes For Full Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define classification groups for eventual classification analysis\n",
    "#create 6 vocabulary sets for class divisions based on common vocab words\n",
    "\n",
    "logic=['logic', 'logical', 'logics', 'syllogism', 'syllogisms', 'model']\n",
    "mathematics=['mathematics','mathematical', 'number', 'set', 'sets', 'probability','probabilities', 'proof']\n",
    "language=['language', 'linguistic', 'sentences', 'sentence', 'proposition', 'propositions', 'verb', 'verbs',\n",
    "         'discourse', 'word', 'words']\n",
    "mind=['cognition', 'cognitive', 'consciousness', 'thought', 'thoughts','knowledge', 'know', 'mental', \n",
    "        'perception', 'neural', 'brain', 'mind', 'selfknowledge']\n",
    "phenomenology=['objects', 'object', 'truth', 'abstract', 'abstraction', 'phenomenal', 'phenomenology',\n",
    "             'representation', 'representational', 'representations', 'experience', 'experiences']\n",
    "ethics=['ethics', 'ethical', 'moral', 'morality', 'religion']\n",
    "\n",
    "top_vocab=[]\n",
    "\n",
    "def merge_list(group):\n",
    "    for word in group:\n",
    "        top_vocab.append(word)\n",
    "\n",
    "merge_list(logic)\n",
    "merge_list(mathematics)\n",
    "merge_list(language)\n",
    "merge_list(mind)\n",
    "merge_list(phenomenology)\n",
    "merge_list(ethics)\n",
    "\n",
    "len(top_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting corpus document size:  1029\n",
      "Full corpus filtered vocab size: 2901\n"
     ]
    }
   ],
   "source": [
    "#load individual corpus docs\n",
    "#Create dict of entire corpus\n",
    "#This will be used to process and align categories, top vocab terms with docs\n",
    "\n",
    "def load_doc(filename):\n",
    "    file=open(filename, 'r')\n",
    "    text=file.read()\n",
    "    file.close()\n",
    "    return text    \n",
    "\n",
    "def clean_doc(doc):\n",
    "    tokens=doc.split()\n",
    "    tokens=[word.lower() for word in tokens]\n",
    "    re_punc=re.compile('[%s]'% re.escape(string.punctuation))\n",
    "    tokens=[re_punc.sub('',w) for w in tokens]\n",
    "    tokens=[word for word in tokens if word.isalpha()]\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    tokens=[word for word in tokens if not word in stop_words]\n",
    "    tokens=[word for word in tokens if len(word)>2]\n",
    "    return tokens\n",
    "\n",
    "def process_docs(directory):\n",
    "    for filename in listdir(directory):\n",
    "        path=directory+'/'+filename\n",
    "        doc=load_doc(path)\n",
    "        tokens=clean_doc(doc)\n",
    "        \n",
    "        #process lists, counters, dicts:\n",
    "        vocab.update(tokens)\n",
    "        wordcount=Counter(tokens)\n",
    "        corpus_dict_top5[filename]=wordcount.most_common(5)\n",
    "        line= ' '.join(tokens)\n",
    "        corpus_dict_sent[filename]=line\n",
    "        vocab_tokens=[word for word in tokens if word in top_vocab]\n",
    "        vocabcount=Counter(vocab_tokens)\n",
    "        corpus_vdict[filename]=vocabcount\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    data='\\n'.join(lines)\n",
    "    file=open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "vocab=Counter()\n",
    "\n",
    "#Top 5 words in each document for categorization\n",
    "corpus_dict_top5={}\n",
    "#Dict with top wordcounts for top vocab 6 group categorization list\n",
    "corpus_vdict={}\n",
    "#Dict with full length sentences\n",
    "corpus_dict_sent={}\n",
    "\n",
    "process_docs(corpus_directory)\n",
    "print('starting corpus document size: ', len(corpus_dict_sent))\n",
    "\n",
    "min_occurrence=50\n",
    "vocab=[k for k,c in vocab.items() if c >= min_occurrence]\n",
    "save_list(vocab, 'vocab.txt')\n",
    "print('Full corpus filtered vocab size: %d' % len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs categorized into groups based on top 5 words in lists:\n",
      "mind             189\n",
      "logic            139\n",
      "phenomenology    135\n",
      "language         128\n",
      "mathematics       95\n",
      "ethics            55\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#create subset of docs for class labels based on category specific vocabulary\n",
    "#merge with main corpus to align docs with class labels\n",
    "#concatenate word within category with corresponding usage frequency\n",
    "\n",
    "def group_dfs(group):\n",
    "    group_set={}\n",
    "    for k,v in corpus_dict_top5.items():\n",
    "        for name, count in v:\n",
    "            if name in group:\n",
    "                group_set[k]=v\n",
    "    dkeys=[]\n",
    "    dvals=[]\n",
    "    for x,y in group_set.items():\n",
    "        dkeys.append(x)\n",
    "        dv=[]\n",
    "        for item in y:\n",
    "            dv.append(str(item[0]+'-'+str(item[1])))\n",
    "        dvals.append(dv)\n",
    "    headers=[]\n",
    "    for x in range(1,6):\n",
    "        label=str('word'+str(x))\n",
    "        headers.append(label)\n",
    "    newdf=pd.DataFrame(dvals, columns=headers, index=dkeys)\n",
    "    return newdf\n",
    "\n",
    "logic_df=group_dfs(logic)\n",
    "mathematics_df=group_dfs(mathematics)\n",
    "language_df=group_dfs(language)\n",
    "mind_df=group_dfs(mind)\n",
    "phenomenology_df=group_dfs(phenomenology)\n",
    "ethics_df=group_dfs(ethics)\n",
    "\n",
    "logic_df['class']='logic'\n",
    "mathematics_df['class']='mathematics'\n",
    "language_df['class']='language'\n",
    "mind_df['class']='mind'\n",
    "phenomenology_df['class']='phenomenology'\n",
    "ethics_df['class']='ethics'\n",
    "\n",
    "frames=[logic_df, mathematics_df, language_df, mind_df, phenomenology_df, ethics_df]\n",
    "full_df=pd.concat(frames)\n",
    "full_df=full_df.reset_index()\n",
    "full_df=full_df.rename(columns={\"index\":\"document\"})\n",
    "full_df.to_csv('full_concat.csv')\n",
    "    \n",
    "print('Docs categorized into groups based on top 5 words in lists:')\n",
    "print(full_df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full df shape with dups:  (741, 7)\n",
      "unique docs shape:  (482, 7)\n",
      "dups docs shape:  (259, 7)\n",
      "\n",
      "Value counts of unique doc classes:\n",
      "mind             141\n",
      "logic             84\n",
      "phenomenology     75\n",
      "language          72\n",
      "mathematics       66\n",
      "ethics            44\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Some docs have vocabulary words from multiple lists\n",
    "#Identify Dups, separate class assignment based on word list class\n",
    "\n",
    "dups=full_df.duplicated(subset=['document'])\n",
    "df_dup=pd.concat([full_df['document'], dups],axis=1, join='inner')\n",
    "df_dup=df_dup.rename(columns={0:'duplicate'})\n",
    "df_dup=df_dup[df_dup.duplicate]\n",
    "\n",
    "duplist=df_dup.document.to_list()\n",
    "uniquedocs=full_df[~full_df.document.isin(duplist)]\n",
    "dupdocs=full_df[full_df.document.isin(duplist)]\n",
    "\n",
    "print('full df shape with dups: ', full_df.shape)\n",
    "print('unique docs shape: ', uniquedocs.shape)\n",
    "print('dups docs shape: ', dupdocs.shape)\n",
    "print('\\nValue counts of unique doc classes:')\n",
    "print(uniquedocs['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "additional unique filtered docs to include:  (64, 7)\n",
      "final corpus shape:  (546, 7)\n",
      "final corpus counts: \n",
      "\n",
      "mind             141\n",
      "logic             84\n",
      "phenomenology     75\n",
      "language          72\n",
      "mathematics       66\n",
      "ethics            44\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Clean duplicate docs based on first word assignment to category\n",
    "#word that is top ranked in category is assigned to that category\n",
    "\n",
    "docfilter=dupdocs.drop_duplicates(subset='document').copy(deep=True)\n",
    "docfilter2=docfilter.copy(deep=True)\n",
    "docfilter2[['word1w','word1c']]=docfilter2.word1.str.split(\"-\",expand=True)\n",
    "\n",
    "keepcols=['document','word1w']\n",
    "primaryword=docfilter2.filter(items=keepcols, axis=1).copy(deep=True)\n",
    "\n",
    "NaN=np.nan\n",
    "primaryword['class']=NaN\n",
    "\n",
    "def firstword(group, name):\n",
    "    for i in range(len(primaryword)):\n",
    "        for word in group:\n",
    "            if primaryword.iloc[i,1]==word:\n",
    "                primaryword.iloc[i,2]=name\n",
    "\n",
    "firstword(logic, 'logic')\n",
    "firstword(mathematics, 'mathematics')\n",
    "firstword(language, 'language')\n",
    "firstword(mind, 'mind')\n",
    "firstword(phenomenology, 'phenomenology')\n",
    "firstword(ethics, 'ethics')\n",
    "\n",
    "primaryword.dropna(subset=['class'], inplace=True)\n",
    "docfilter=docfilter.drop(columns=['class'])\n",
    "uniquefiltered=pd.concat([docfilter, primaryword['class']],axis=1, join='inner')\n",
    "print('additional unique filtered docs to include: ', uniquefiltered.shape)\n",
    "\n",
    "frames=[uniquedocs, uniquefiltered]\n",
    "final_corpus_df=pd.concat(frames)\n",
    "print('final corpus shape: ', final_corpus_df.shape)\n",
    "print('final corpus counts: \\n')\n",
    "print(uniquedocs['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Classification Approaches\n",
    "# Heuristic Classification, TF-IDF, Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Heuristic Judgment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selfknowledge</th>\n",
       "      <th>knowledge</th>\n",
       "      <th>thought</th>\n",
       "      <th>cognitive</th>\n",
       "      <th>mental</th>\n",
       "      <th>know</th>\n",
       "      <th>thoughts</th>\n",
       "      <th>consciousness</th>\n",
       "      <th>phenomenal</th>\n",
       "      <th>experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Self Knowledge_1 The Distinctiveness of SelfKnowledge.txt</th>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Folk Psychology as Mental Simulation_7 Conclusion.txt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gertrude Elizabeth Margaret Anscombe_3 Metaphysics.txt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consciousness and Intentionality_8 Consciousness in Mind.txt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Thomas Reid_6 Moral Philosophy.txt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Folk Psychology as Mental Simulation_6 Simulation Theory Pros and Cons.txt</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Narrow Mental Content_3 Arguments for Narrow Content.txt</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Externalism About Mental Content_6 Externalism and Selfknowledge.txt</th>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self Knowledge_2 Doubts about the distinctiveness of selfknowledge.txt</th>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Self Knowledge_3 Accounts of SelfKnowledge.txt</th>\n",
       "      <td>73.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    selfknowledge  knowledge  \\\n",
       "Self Knowledge_1 The Distinctiveness of SelfKno...           18.0        5.0   \n",
       "Folk Psychology as Mental Simulation_7 Conclusi...            1.0        1.0   \n",
       "Gertrude Elizabeth Margaret Anscombe_3 Metaphys...            1.0        NaN   \n",
       "Consciousness and Intentionality_8 Consciousnes...            1.0        NaN   \n",
       "Thomas Reid_6 Moral Philosophy.txt                            1.0        4.0   \n",
       "Folk Psychology as Mental Simulation_6 Simulati...            2.0        6.0   \n",
       "Narrow Mental Content_3 Arguments for Narrow Co...            1.0        1.0   \n",
       "Externalism About Mental Content_6 Externalism ...           15.0       11.0   \n",
       "Self Knowledge_2 Doubts about the distinctivene...           10.0        5.0   \n",
       "Self Knowledge_3 Accounts of SelfKnowledge.txt               73.0       31.0   \n",
       "\n",
       "                                                    thought  cognitive  \\\n",
       "Self Knowledge_1 The Distinctiveness of SelfKno...      2.0        1.0   \n",
       "Folk Psychology as Mental Simulation_7 Conclusi...      NaN        3.0   \n",
       "Gertrude Elizabeth Margaret Anscombe_3 Metaphys...      1.0        NaN   \n",
       "Consciousness and Intentionality_8 Consciousnes...     10.0        5.0   \n",
       "Thomas Reid_6 Moral Philosophy.txt                      2.0        NaN   \n",
       "Folk Psychology as Mental Simulation_6 Simulati...      NaN        4.0   \n",
       "Narrow Mental Content_3 Arguments for Narrow Co...      5.0        NaN   \n",
       "Externalism About Mental Content_6 Externalism ...      7.0        NaN   \n",
       "Self Knowledge_2 Doubts about the distinctivene...      4.0        1.0   \n",
       "Self Knowledge_3 Accounts of SelfKnowledge.txt          4.0        6.0   \n",
       "\n",
       "                                                    mental  know  thoughts  \\\n",
       "Self Knowledge_1 The Distinctiveness of SelfKno...    19.0   3.0       6.0   \n",
       "Folk Psychology as Mental Simulation_7 Conclusi...     2.0   NaN       NaN   \n",
       "Gertrude Elizabeth Margaret Anscombe_3 Metaphys...     NaN   NaN       NaN   \n",
       "Consciousness and Intentionality_8 Consciousnes...     6.0   1.0       1.0   \n",
       "Thomas Reid_6 Moral Philosophy.txt                     2.0   1.0       NaN   \n",
       "Folk Psychology as Mental Simulation_6 Simulati...    25.0   2.0       NaN   \n",
       "Narrow Mental Content_3 Arguments for Narrow Co...    10.0   NaN      10.0   \n",
       "Externalism About Mental Content_6 Externalism ...     1.0   9.0      14.0   \n",
       "Self Knowledge_2 Doubts about the distinctivene...     8.0   7.0       NaN   \n",
       "Self Knowledge_3 Accounts of SelfKnowledge.txt        49.0  15.0       8.0   \n",
       "\n",
       "                                                    consciousness  phenomenal  \\\n",
       "Self Knowledge_1 The Distinctiveness of SelfKno...            1.0         2.0   \n",
       "Folk Psychology as Mental Simulation_7 Conclusi...            NaN         NaN   \n",
       "Gertrude Elizabeth Margaret Anscombe_3 Metaphys...            NaN         NaN   \n",
       "Consciousness and Intentionality_8 Consciousnes...           27.0         5.0   \n",
       "Thomas Reid_6 Moral Philosophy.txt                            NaN         NaN   \n",
       "Folk Psychology as Mental Simulation_6 Simulati...            NaN         NaN   \n",
       "Narrow Mental Content_3 Arguments for Narrow Co...            NaN        23.0   \n",
       "Externalism About Mental Content_6 Externalism ...            NaN         NaN   \n",
       "Self Knowledge_2 Doubts about the distinctivene...            NaN         NaN   \n",
       "Self Knowledge_3 Accounts of SelfKnowledge.txt                4.0        12.0   \n",
       "\n",
       "                                                    experience  \n",
       "Self Knowledge_1 The Distinctiveness of SelfKno...         2.0  \n",
       "Folk Psychology as Mental Simulation_7 Conclusi...         NaN  \n",
       "Gertrude Elizabeth Margaret Anscombe_3 Metaphys...         NaN  \n",
       "Consciousness and Intentionality_8 Consciousnes...         2.0  \n",
       "Thomas Reid_6 Moral Philosophy.txt                         1.0  \n",
       "Folk Psychology as Mental Simulation_6 Simulati...         1.0  \n",
       "Narrow Mental Content_3 Arguments for Narrow Co...         5.0  \n",
       "Externalism About Mental Content_6 Externalism ...         NaN  \n",
       "Self Knowledge_2 Doubts about the distinctivene...         1.0  \n",
       "Self Knowledge_3 Accounts of SelfKnowledge.txt            11.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_texts=final_corpus_df['document']\n",
    "final_texts=final_texts.to_list()\n",
    "\n",
    "final_vcorpus={}\n",
    "\n",
    "for k,v in corpus_vdict.items():\n",
    "    for word in final_texts:\n",
    "        if k == word:\n",
    "            final_vcorpus[k]=v\n",
    "\n",
    "vocab_matrix=pd.DataFrame.from_dict(final_vcorpus, orient='index')\n",
    "vocab_matrix.to_csv('vocab_matrix_analyst.csv')\n",
    "vocab_matrix.iloc[:10,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(546, 3000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "final_corpus=[]\n",
    "final_labels=[]\n",
    "\n",
    "for k,v in corpus_dict_sent.items():\n",
    "    for word in final_texts:\n",
    "        if k == word:\n",
    "            final_corpus.append(v)\n",
    "            final_labels.append(k)\n",
    "\n",
    "vectorizer=TfidfVectorizer(max_features=3000)\n",
    "X=vectorizer.fit_transform(final_corpus)\n",
    "print(X.shape)\n",
    "\n",
    "feature_names=vectorizer.get_feature_names()\n",
    "corpus_index=[n for n in final_corpus]\n",
    "Tfidf_df_matrix=pd.DataFrame(X.todense(), index=final_labels, columns=feature_names)\n",
    "Tfidf_df_matrix.T.to_csv('tfidf_vocab_matrix_full.csv')\n",
    "\n",
    "Tfidf_df_matrix_topVocab=Tfidf_df_matrix[top_vocab]\n",
    "Tfidf_df_matrix_topVocab.iloc[:10,:10]\n",
    "Tfidf_df_matrix_topVocab.to_csv('Tfidf_matrix_topVocab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items in X set: 546 and items in y set: 546\n"
     ]
    }
   ],
   "source": [
    "#extract data from overall corpus and split into train test for modelling\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "doc_class_df=final_corpus_df[['document','class']]\n",
    "final_texts=doc_class_df.values.tolist()\n",
    "\n",
    "final_corpus_dict={}\n",
    "final_labels_dict={}\n",
    "final_analysis_dict={}\n",
    "\n",
    "for k,v in corpus_dict_sent.items():\n",
    "    for item in final_texts:\n",
    "        if k == item[0]:\n",
    "            final_corpus_dict[k]=v\n",
    "            final_labels_dict[k]=item[1]\n",
    "            final_analysis_dict[v]=item[1]\n",
    "\n",
    "\n",
    "with open ('/home/meeka/Desktop/NU/453/assn3/json_texts_orig.json', 'w') as write_file:\n",
    "    json.dump(final_corpus_dict, write_file)\n",
    "with open ('/home/meeka/Desktop/NU/453/assn3/json_texts_labels.json', 'w') as write_file:\n",
    "    json.dump(final_analysis_dict, write_file)      \n",
    "                 \n",
    "X=list(final_analysis_dict.keys())\n",
    "y=list(final_analysis_dict.values())   \n",
    "print('items in X set: %d and items in y set: %d' %(len(X), len(y)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=100, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logic</th>\n",
       "      <th>logical</th>\n",
       "      <th>logics</th>\n",
       "      <th>syllogism</th>\n",
       "      <th>syllogisms</th>\n",
       "      <th>model</th>\n",
       "      <th>mathematics</th>\n",
       "      <th>mathematical</th>\n",
       "      <th>number</th>\n",
       "      <th>set</th>\n",
       "      <th>...</th>\n",
       "      <th>representation</th>\n",
       "      <th>representational</th>\n",
       "      <th>representations</th>\n",
       "      <th>experience</th>\n",
       "      <th>experiences</th>\n",
       "      <th>ethics</th>\n",
       "      <th>ethical</th>\n",
       "      <th>moral</th>\n",
       "      <th>morality</th>\n",
       "      <th>religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mathematics</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mind</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mathematics</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mind</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phenomenology</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               logic  logical  logics  syllogism  syllogisms  model  \\\n",
       "mathematics        0        0       0          0           0      0   \n",
       "mind               0        1       0          0           0      1   \n",
       "mathematics        2        0       0          0           0      0   \n",
       "mind               1        0       0          0           0      0   \n",
       "phenomenology      0        0       0          0           0      0   \n",
       "\n",
       "               mathematics  mathematical  number  set  ...  representation  \\\n",
       "mathematics              0             1       2    6  ...               0   \n",
       "mind                     0             0       3    0  ...              17   \n",
       "mathematics              1             0       0   16  ...               0   \n",
       "mind                     1             0       0    0  ...               0   \n",
       "phenomenology            0             0       0    0  ...               0   \n",
       "\n",
       "               representational  representations  experience  experiences  \\\n",
       "mathematics                   0                0           0            0   \n",
       "mind                          1               28           0            0   \n",
       "mathematics                   0                0           0            0   \n",
       "mind                          0                0           2            1   \n",
       "phenomenology                 0                0           0            0   \n",
       "\n",
       "               ethics  ethical  moral  morality  religion  \n",
       "mathematics         0        0      0         0         0  \n",
       "mind                0        0      2         0         0  \n",
       "mathematics         0        0      0         0         0  \n",
       "mind                0        0      0         0         0  \n",
       "phenomenology       0        0      0         0         0  \n",
       "\n",
       "[5 rows x 55 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analysis 1: Best Word Judgment\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "analyst_vectorizer=CountVectorizer(vocabulary=top_vocab)\n",
    "X_train_analyst=analyst_vectorizer.fit_transform(X_train)\n",
    "X_test_analyst=analyst_vectorizer.fit_transform(X_test)\n",
    "\n",
    "#Analyst Dataframe\n",
    "X_train_analyst_df=pd.DataFrame(X_train_analyst.todense(), columns=analyst_vectorizer.get_feature_names(), index=y_train)\n",
    "X_train_analyst_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>special</th>\n",
       "      <th>selfknowledge</th>\n",
       "      <th>compared</th>\n",
       "      <th>knowledge</th>\n",
       "      <th>domains</th>\n",
       "      <th>thought</th>\n",
       "      <th>differ</th>\n",
       "      <th>sorts</th>\n",
       "      <th>one</th>\n",
       "      <th>following</th>\n",
       "      <th>...</th>\n",
       "      <th>rfm</th>\n",
       "      <th>oscar</th>\n",
       "      <th>proportionality</th>\n",
       "      <th>kappa</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>civilians</th>\n",
       "      <th>kosslyn</th>\n",
       "      <th>pit</th>\n",
       "      <th>biv</th>\n",
       "      <th>combatants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mathematics</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mind</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mathematics</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mind</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phenomenology</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2901 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               special  selfknowledge  compared  knowledge  domains  thought  \\\n",
       "mathematics          0              0         0          0        0        0   \n",
       "mind                 0              0         0          2        0       12   \n",
       "mathematics          0              0         0          0        0        0   \n",
       "mind                 0              0         0         19        0        3   \n",
       "phenomenology        0              0         0          2        0        0   \n",
       "\n",
       "               differ  sorts  one  following  ...  rfm  oscar  \\\n",
       "mathematics         0      0    0          2  ...    0      0   \n",
       "mind                1      3   17          0  ...    0      0   \n",
       "mathematics         0      0    1          0  ...    0      0   \n",
       "mind                0      0    2          1  ...    0      0   \n",
       "phenomenology       0      0    0          0  ...    0      0   \n",
       "\n",
       "               proportionality  kappa  epsilon  civilians  kosslyn  pit  biv  \\\n",
       "mathematics                  0      0        0          0        0    0    0   \n",
       "mind                         0      0        0          0        0    0    0   \n",
       "mathematics                  0      0        0          0        0    0    0   \n",
       "mind                         0      0        0          0        0    0    0   \n",
       "phenomenology                0      0        0          0        0    0    0   \n",
       "\n",
       "               combatants  \n",
       "mathematics             0  \n",
       "mind                    0  \n",
       "mathematics             0  \n",
       "mind                    0  \n",
       "phenomenology           0  \n",
       "\n",
       "[5 rows x 2901 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analysis 1.5: Full corpus\n",
    "\n",
    "analyst_vectorizer2=CountVectorizer(vocabulary=vocab)\n",
    "X_train_analyst2=analyst_vectorizer2.fit_transform(X_train)\n",
    "X_test_analyst2=analyst_vectorizer2.fit_transform(X_test)\n",
    "\n",
    "#Analyst 2 Dataframe\n",
    "X_train_analyst_df2=pd.DataFrame(X_train_analyst2.todense(), columns=analyst_vectorizer2.get_feature_names(), index=y_train)\n",
    "X_train_analyst_df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>special</th>\n",
       "      <th>selfknowledge</th>\n",
       "      <th>compared</th>\n",
       "      <th>knowledge</th>\n",
       "      <th>domains</th>\n",
       "      <th>thought</th>\n",
       "      <th>differ</th>\n",
       "      <th>sorts</th>\n",
       "      <th>one</th>\n",
       "      <th>following</th>\n",
       "      <th>...</th>\n",
       "      <th>rfm</th>\n",
       "      <th>oscar</th>\n",
       "      <th>proportionality</th>\n",
       "      <th>kappa</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>civilians</th>\n",
       "      <th>kosslyn</th>\n",
       "      <th>pit</th>\n",
       "      <th>biv</th>\n",
       "      <th>combatants</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mathematics</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mind</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085538</td>\n",
       "      <td>0.012057</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.082719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mathematics</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mind</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.240317</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013613</td>\n",
       "      <td>0.011195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phenomenology</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112383</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2901 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               special  selfknowledge  compared  knowledge  domains   thought  \\\n",
       "mathematics        0.0            0.0       0.0   0.000000      0.0  0.000000   \n",
       "mind               0.0            0.0       0.0   0.018084      0.0  0.085538   \n",
       "mathematics        0.0            0.0       0.0   0.000000      0.0  0.000000   \n",
       "mind               0.0            0.0       0.0   0.240317      0.0  0.029914   \n",
       "phenomenology      0.0            0.0       0.0   0.112383      0.0  0.000000   \n",
       "\n",
       "                 differ  sorts       one  following  ...  rfm  oscar  \\\n",
       "mathematics    0.000000  0.000  0.000000   0.034746  ...  0.0    0.0   \n",
       "mind           0.012057  0.036  0.082719   0.000000  ...  0.0    0.0   \n",
       "mathematics    0.000000  0.000  0.016069   0.000000  ...  0.0    0.0   \n",
       "mind           0.000000  0.000  0.013613   0.011195  ...  0.0    0.0   \n",
       "phenomenology  0.000000  0.000  0.000000   0.000000  ...  0.0    0.0   \n",
       "\n",
       "               proportionality  kappa  epsilon  civilians  kosslyn  pit  biv  \\\n",
       "mathematics                0.0    0.0      0.0        0.0      0.0  0.0  0.0   \n",
       "mind                       0.0    0.0      0.0        0.0      0.0  0.0  0.0   \n",
       "mathematics                0.0    0.0      0.0        0.0      0.0  0.0  0.0   \n",
       "mind                       0.0    0.0      0.0        0.0      0.0  0.0  0.0   \n",
       "phenomenology              0.0    0.0      0.0        0.0      0.0  0.0  0.0   \n",
       "\n",
       "               combatants  \n",
       "mathematics           0.0  \n",
       "mind                  0.0  \n",
       "mathematics           0.0  \n",
       "mind                  0.0  \n",
       "phenomenology         0.0  \n",
       "\n",
       "[5 rows x 2901 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analysis 2: Tf-Idf Dataframe\n",
    "\n",
    "#tfidf_vectorizer=TfidfVectorizer(max_features=1000)\n",
    "tfidf_vectorizer=TfidfVectorizer(vocabulary=vocab)\n",
    "X_train_tfidf=tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf=tfidf_vectorizer.fit_transform(X_test)\n",
    "\n",
    "X_train_tfidf_df=pd.DataFrame(X_train_tfidf.todense(), columns=tfidf_vectorizer.get_feature_names(), index=y_train)\n",
    "X_train_tfidf_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 3:  NN Embeddings (Doc2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446, 50)\n",
      "(100, 50)\n"
     ]
    }
   ],
   "source": [
    "#Analysis 3: Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "def tokenize_docs(X):\n",
    "    word_tokens=[]\n",
    "    for doc in X:\n",
    "        tokens=doc.split()\n",
    "        word_tokens.append(tokens)\n",
    "    return word_tokens\n",
    "    \n",
    "X_train_tokens=tokenize_docs(X_train)\n",
    "X_test_tokens=tokenize_docs(X_test)\n",
    "\n",
    "#50 Dimensions\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(X_train_tokens)]\n",
    "model_50dim = Doc2Vec(documents, vector_size=50, window=4, min_count=2, epochs=50)\n",
    "model_50dim.train(documents, total_examples = model_50dim.corpus_count, epochs = model_50dim.epochs)\n",
    "\n",
    "#Vectorize Training Set:\n",
    "doc2vec_50_vectors = np.zeros((len(X_train_tokens), 50)) \n",
    "for i in range(0, len(X_train_tokens)):\n",
    "    doc2vec_50_vectors[i,] = model_50dim.infer_vector(X_train_tokens[i]).transpose()\n",
    "print(doc2vec_50_vectors.shape)\n",
    "\n",
    "#Vectorize Test Set:\n",
    "doc2vec_50_vectors_test = np.zeros((len(X_test_tokens), 50))\n",
    "for i in range(0, len(X_test_tokens)):\n",
    "    doc2vec_50_vectors_test[i,] = model_50dim.infer_vector(X_test_tokens[i]).transpose()\n",
    "print(doc2vec_50_vectors_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446, 150)\n",
      "(100, 150)\n"
     ]
    }
   ],
   "source": [
    "#150 Dimensions\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(X_train_tokens)]\n",
    "model_150dim = Doc2Vec(documents, vector_size=150, window=4, min_count=2, epochs=50)\n",
    "model_150dim.train(documents, total_examples = model_150dim.corpus_count, epochs = model_150dim.epochs)\n",
    "\n",
    "#Vectorize Training Set:\n",
    "doc2vec_150_vectors = np.zeros((len(X_train_tokens), 150)) \n",
    "for i in range(0, len(X_train_tokens)):\n",
    "    doc2vec_150_vectors[i,] = model_150dim.infer_vector(X_train_tokens[i]).transpose()\n",
    "print(doc2vec_150_vectors.shape)\n",
    "\n",
    "#Vectorize Test Set:\n",
    "doc2vec_150_vectors_test = np.zeros((len(X_test_tokens), 150))\n",
    "for i in range(0, len(X_test_tokens)):\n",
    "    doc2vec_150_vectors_test[i,] = model_150dim.infer_vector(X_test_tokens[i]).transpose()\n",
    "print(doc2vec_150_vectors_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446, 200)\n",
      "(100, 200)\n"
     ]
    }
   ],
   "source": [
    "#200 Dimensions\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(X_train_tokens)]\n",
    "model_200dim = Doc2Vec(documents, vector_size=200, window=4, min_count=2, epochs=50)\n",
    "model_200dim.train(documents, total_examples = model_150dim.corpus_count, epochs = model_150dim.epochs)\n",
    "\n",
    "#Vectorize Training Set:\n",
    "doc2vec_200_vectors = np.zeros((len(X_train_tokens), 200)) \n",
    "for i in range(0, len(X_train_tokens)):\n",
    "    doc2vec_200_vectors[i,] = model_200dim.infer_vector(X_train_tokens[i]).transpose()\n",
    "print(doc2vec_200_vectors.shape)\n",
    "\n",
    "#Vectorize Test Set:\n",
    "doc2vec_200_vectors_test = np.zeros((len(X_test_tokens), 200))\n",
    "for i in range(0, len(X_test_tokens)):\n",
    "    doc2vec_200_vectors_test[i,] = model_200dim.infer_vector(X_test_tokens[i]).transpose()\n",
    "print(doc2vec_200_vectors_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Model Performance Results with Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyst/Random forest F1 classification performance in test set: 0.778\n",
      "\n",
      "Full Doc Vectorization/Random forest F1 classification performance in test set: 0.659\n",
      "\n",
      "TF-IDF/Random forest F1 classification performance in test set: 0.654\n",
      "\n",
      "Doc2Vec_50/Random forest F1 classification performance in test set: 0.651\n",
      "\n",
      "Doc2Vec_150/Random forest F1 classification performance in test set: 0.528\n",
      "\n",
      "Doc2Vec_200/Random forest F1 classification performance in test set: 0.583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "#Analyst Judgement #1\n",
    "count_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "count_clf.fit(X_train_analyst, y_train)\n",
    "count_pred = count_clf.predict(X_test_analyst)\n",
    "print('\\nAnalyst/Random forest F1 classification performance in test set:',\n",
    "    round(metrics.f1_score(y_test, count_pred, average='macro'), 3))\n",
    "\n",
    "#Full Document Vectorization\n",
    "count2_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "count2_clf.fit(X_train_analyst2, y_train)\n",
    "count2_pred = count2_clf.predict(X_test_analyst2)\n",
    "print('\\nFull Doc Vectorization/Random forest F1 classification performance in test set:',\n",
    "    round(metrics.f1_score(y_test, count2_pred, average='macro'), 3))\n",
    "\n",
    "#Tf-Idf\n",
    "tfidf_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "tfidf_clf.fit(X_train_tfidf, y_train)\n",
    "tfidf_pred = tfidf_clf.predict(X_test_tfidf)\n",
    "print('\\nTF-IDF/Random forest F1 classification performance in test set:',\n",
    "    round(metrics.f1_score(y_test, tfidf_pred, average='macro'), 3))\n",
    "\n",
    "#Doc2Vec 50\n",
    "doc2vec_50_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "doc2vec_50_clf.fit(doc2vec_50_vectors, y_train)\n",
    "doc2vec_50_pred = doc2vec_50_clf.predict(doc2vec_50_vectors_test)\n",
    "print('\\nDoc2Vec_50/Random forest F1 classification performance in test set:',\n",
    "    round(metrics.f1_score(y_test, doc2vec_50_pred, average='macro'), 3)) \n",
    "\n",
    "#Doc2Vec 150\n",
    "doc2vec_150_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "doc2vec_150_clf.fit(doc2vec_150_vectors, y_train)\n",
    "doc2vec_150_pred = doc2vec_150_clf.predict(doc2vec_150_vectors_test)\n",
    "print('\\nDoc2Vec_150/Random forest F1 classification performance in test set:',\n",
    "    round(metrics.f1_score(y_test, doc2vec_150_pred, average='macro'), 3)) \n",
    "\n",
    "#Doc2Vec 200\n",
    "doc2vec_200_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "doc2vec_200_clf.fit(doc2vec_200_vectors, y_train)\n",
    "doc2vec_200_pred = doc2vec_200_clf.predict(doc2vec_200_vectors_test)\n",
    "print('\\nDoc2Vec_200/Random forest F1 classification performance in test set:',\n",
    "    round(metrics.f1_score(y_test, doc2vec_200_pred, average='macro'), 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2893 \n",
      "Maximum length: 5329 \n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 5329, 100)         289300    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 5322, 128)         102528    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 2661, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 340608)            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                21798976  \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 22,191,194\n",
      "Trainable params: 22,191,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "14/14 - 1s - loss: 2.1953 - accuracy: 0.2018\n",
      "Epoch 2/10\n",
      "14/14 - 1s - loss: 1.5573 - accuracy: 0.3161\n",
      "Epoch 3/10\n",
      "14/14 - 1s - loss: 1.2105 - accuracy: 0.5112\n",
      "Epoch 4/10\n",
      "14/14 - 1s - loss: 0.7413 - accuracy: 0.7511\n",
      "Epoch 5/10\n",
      "14/14 - 1s - loss: 0.3370 - accuracy: 0.9126\n",
      "Epoch 6/10\n",
      "14/14 - 1s - loss: 0.1293 - accuracy: 0.9664\n",
      "Epoch 7/10\n",
      "14/14 - 1s - loss: 0.0424 - accuracy: 0.9955\n",
      "Epoch 8/10\n",
      "14/14 - 1s - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "14/14 - 1s - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "14/14 - 1s - loss: 0.0039 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7ec426fad0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "def clean_doc(corpus, vocab):\n",
    "    slim_doc=[]\n",
    "    for doc in corpus:\n",
    "        tokens = doc.split()\n",
    "        tokens = [w for w in tokens if w in vocab]\n",
    "        tokens = ' '.join(tokens)\n",
    "        slim_doc.append(tokens)\n",
    "    return slim_doc\n",
    "\n",
    "X_train_slim=clean_doc(X_train, vocab)\n",
    "X_test_slim=clean_doc(X_test, vocab)\n",
    "\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding= 'post')\n",
    "    return padded\n",
    "\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(128, 8, activation= 'relu' ))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation= 'relu'))\n",
    "    model.add(Dense(6, activation= 'softmax'))\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "tokenizer = create_tokenizer(X_train_slim)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print( 'Vocabulary size: %d ' % vocab_size)\n",
    "\n",
    "max_length = max([len(s.split()) for s in X_train_slim])\n",
    "print( 'Maximum length: %d ' % max_length)\n",
    "\n",
    "Xtrain_s = encode_docs(tokenizer, max_length, X_train_slim)\n",
    "Xtest_s = encode_docs(tokenizer, max_length, X_test_slim)\n",
    "\n",
    "le = LabelEncoder()\n",
    "ytrain = le.fit_transform(y_train)\n",
    "ytrain=to_categorical(ytrain)\n",
    "\n",
    "ytest=le.fit_transform(y_test)\n",
    "ytest=to_categorical(ytest)\n",
    "\n",
    "basic_model = define_model(vocab_size, max_length)\n",
    "basic_model.fit(Xtrain_s, ytrain, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Accuracy: 62.00 \n"
     ]
    }
   ],
   "source": [
    "_, acc = basic_model.evaluate(Xtest_s, ytest, verbose=0)\n",
    "print(' Test Accuracy: %.2f ' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.67      0.50         6\n",
      "           1       0.45      0.59      0.51        17\n",
      "           2       0.61      0.74      0.67        19\n",
      "           3       0.91      0.77      0.83        13\n",
      "           4       0.67      0.64      0.65        25\n",
      "           5       0.80      0.40      0.53        20\n",
      "\n",
      "    accuracy                           0.62       100\n",
      "   macro avg       0.64      0.63      0.62       100\n",
      "weighted avg       0.66      0.62      0.62       100\n",
      "\n",
      "Encoded label order 0:5:  ['ethics', 'language', 'logic', 'mathematics', 'mind', 'ontology']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "le = LabelEncoder()\n",
    "ytest = le.fit_transform(y_test)\n",
    "\n",
    "\n",
    "y_pred = basic_model.predict(Xtest_s, verbose=1)\n",
    "y_pred_bool = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(ytest, y_pred_bool))\n",
    "print('Encoded label order 0:5: ', list(le.inverse_transform([0,1,2,3,4,5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorial Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis 2: Full Doc Vectorization 50\n",
    "analyst_vectorizer50=CountVectorizer(max_features=50)\n",
    "X_train_analyst50=analyst_vectorizer50.fit_transform(X_train)\n",
    "X_test_analyst50=analyst_vectorizer50.fit_transform(X_test)\n",
    "\n",
    "#Analysis 2: Full Doc Vectorization 150\n",
    "analyst_vectorizer150=CountVectorizer(max_features=150)\n",
    "X_train_analyst150=analyst_vectorizer150.fit_transform(X_train)\n",
    "X_test_analyst150=analyst_vectorizer150.fit_transform(X_test)\n",
    "\n",
    "#Analysis 2: Full Doc Vectorization 200\n",
    "analyst_vectorizer200=CountVectorizer(max_features=200)\n",
    "X_train_analyst200=analyst_vectorizer200.fit_transform(X_train)\n",
    "X_test_analyst200=analyst_vectorizer200.fit_transform(X_test)\n",
    "\n",
    "#tfidf_vectorizer 50\n",
    "tfidf_vectorizer50=TfidfVectorizer(max_features=50)\n",
    "X_train_tfidf50=tfidf_vectorizer50.fit_transform(X_train)\n",
    "X_test_tfidf50=tfidf_vectorizer50.fit_transform(X_test)\n",
    "\n",
    "#tfidf_vectorizer 150\n",
    "tfidf_vectorizer150=TfidfVectorizer(max_features=150)\n",
    "X_train_tfidf150=tfidf_vectorizer150.fit_transform(X_train)\n",
    "X_test_tfidf150=tfidf_vectorizer150.fit_transform(X_test)\n",
    "\n",
    "#tfidf_vectorizer 20\n",
    "tfidf_vectorizer200=TfidfVectorizer(max_features=200)\n",
    "X_train_tfidf200=tfidf_vectorizer200.fit_transform(X_train)\n",
    "X_test_tfidf200=tfidf_vectorizer200.fit_transform(X_test)\n",
    "\n",
    "################################################\n",
    "\n",
    "#Analysis 2: Full Doc Vectorization 50\n",
    "count50_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "\n",
    "count50_clf.fit(X_train_analyst50, y_train)\n",
    "count50_pred = count50_clf.predict(X_test_analyst50)\n",
    "print('\\nFull Doc 50 Vectorization/Random forest F1 classification performance in test set:',\n",
    "    round(metrics.f1_score(y_test, count50_pred, average='macro'), 3))\n",
    "\n",
    "#Analysis 2: Full Doc Vectorization 150\n",
    "count150_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "count150_clf.fit(X_train_analyst150, y_train)\n",
    "count150_pred = count150_clf.predict(X_test_analyst150)\n",
    "print('\\nFull Doc 150 Vectorization/Random forest F1 classification performance in test set:',\n",
    "    round(metrics.f1_score(y_test, count150_pred, average='macro'), 3))\n",
    "\n",
    "#Analysis 2: Full Doc Vectorization 200\n",
    "count200_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "count200_clf.fit(X_train_analyst200, y_train)\n",
    "count200_pred = count200_clf.predict(X_test_analyst200)\n",
    "print('\\nFull Doc 200 Vectorization/Random forest F1 classification performance in test set:',\n",
    "    round(metrics.f1_score(y_test, count200_pred, average='macro'), 3))\n",
    "\n",
    "\n",
    "#Tf-Idf 50\n",
    "tfidf50_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "tfidf50_clf.fit(X_train_tfidf50, y_train)\n",
    "tfidf50_pred = tfidf50_clf.predict(X_test_tfidf50)\n",
    "print('\\nTF-IDF 50/Random forest F1 classification performance in test set:',\n",
    "    round(metrics.f1_score(y_test, tfidf50_pred, average='macro'), 3))\n",
    "\n",
    "#Tf-Idf 150\n",
    "tfidf150_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "tfidf150_clf.fit(X_train_tfidf150, y_train)\n",
    "tfidf150_pred = tfidf150_clf.predict(X_test_tfidf150)\n",
    "print('\\nTF-IDF 150/Random forest F1 classification performance in test set:',\n",
    "    round(metrics.f1_score(y_test, tfidf150_pred, average='macro'), 3))\n",
    "\n",
    "#Tf-Idf 200\n",
    "tfidf200_clf = RandomForestClassifier(n_estimators = 100, max_depth = 10, random_state = 42)\n",
    "tfidf200_clf.fit(X_train_tfidf200, y_train)\n",
    "tfidf200_pred = tfidf200_clf.predict(X_test_tfidf200)\n",
    "print('\\nTF-IDF 200/Random forest F1 classification performance in test set:',\n",
    "    round(metrics.f1_score(y_test, tfidf200_pred, average='macro'), 3))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Language Py3.7",
   "language": "python",
   "name": "language"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
