# NLP

This project is a compilation of multiple processes and components.  In sequence:
Part 1: Scrape the Stanford Encyclopedia of Philosophy, processes individual text paragraphs using word tokenization as 'bag of words'
Part 2:  Quantify individual word importance and relevance using TF-IDF scoring and creates a targeted search function based on cosine similarity of word embeddings.
Part 3:  Unsupervised learning to identify latent document categories and hierarchical structure:  Comparison of Linear Discriminant Analysis (LDA), Latent Dirichlet Allocation (LDiA), K-means, hierarchical clustering
Part 4:  Generate novel/new text using artificial recurrent neural network (RNN) architecure:  Long short-term memory (LSTM) based on LDiA classifications.

Languages & Packages: Python, Unix, Scrapy, BeautifulSoup, Sklearn, NLTK

Source:  https://plato.stanford.edu/
